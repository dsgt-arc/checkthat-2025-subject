{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "from typing import AnyStr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "def to_labels(\n",
    "        data,\n",
    "        threshold=0.5\n",
    "):\n",
    "    ypred = []\n",
    "    for pred in data:\n",
    "        if pred >= threshold:\n",
    "            ypred.append('SUBJ')\n",
    "        else:\n",
    "            ypred.append('OBJ')\n",
    "    return ypred\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "        X,\n",
    "        tokenizer,\n",
    "        max_length,\n",
    "        y=[]\n",
    "):\n",
    "    pad = tf.keras.preprocessing.sequence.pad_sequences  # (seq, padding = 'post', maxlen = maxlen)\n",
    "    tokenizer = tokenizer\n",
    "    data_fields = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"Label\": []\n",
    "    }\n",
    "    labels = {\n",
    "        'SUBJ': 1.0,\n",
    "        'OBJ': 0.0\n",
    "    }\n",
    "    for i in range(len(X)):\n",
    "        data = tokenizer(X[i])\n",
    "        padded = pad([data['input_ids'], data['attention_mask'], data['token_type_ids']], padding='post',\n",
    "                     maxlen=max_length)\n",
    "        data_fields['input_ids'].append(padded[0])\n",
    "        data_fields['attention_mask'].append(padded[1])\n",
    "        data_fields['token_type_ids'].append(padded[-1])\n",
    "    if len(y):\n",
    "        data_fields['label'] = list(map(lambda e: labels[e], y))\n",
    "    else:\n",
    "        data_fields['label'] = None\n",
    "\n",
    "    for key in data_fields:\n",
    "        data_fields[key] = np.array(data_fields[key])\n",
    "\n",
    "    return [data_fields[\"input_ids\"],\n",
    "            data_fields[\"token_type_ids\"],\n",
    "            data_fields[\"attention_mask\"]], data_fields[\"label\"]\n",
    "\n",
    "\n",
    "def run_sbert_lr_baseline(\n",
    "        data_dir: AnyStr,\n",
    "        train_filepath: AnyStr,\n",
    "        test_filepath: AnyStr\n",
    ") -> AnyStr:\n",
    "    train_data = pd.read_csv(train_filepath, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "    test_data = pd.read_csv(test_filepath, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    vect = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "    model = LogisticRegression(class_weight=\"balanced\")\n",
    "    model.fit(X=vect.encode(train_data['sentence'].values), y=train_data['label'].values)\n",
    "\n",
    "    predictions = model.predict(X=vect.encode(test_data['sentence'].values)).tolist()\n",
    "    pred_df = pd.DataFrame()\n",
    "    pred_df['sentence_id'] = test_data['sentence_id']\n",
    "    pred_df['label'] = predictions\n",
    "\n",
    "    predictions_filepath = os.path.join(data_dir, 'base_pred_lan.tsv')\n",
    "    pred_df.to_csv(predictions_filepath, index=False, sep='\\t')\n",
    "\n",
    "    return predictions_filepath\n",
    "\n",
    "\n",
    "def float_formatter(f) -> str:\n",
    "    \"\"\"\n",
    "    Format a float as a pretty string.\n",
    "    \"\"\"\n",
    "    if f != f or f is None:\n",
    "        # instead of returning nan, return \"\" so it shows blank in table\n",
    "        return \"\"\n",
    "    if isinstance(f, int):\n",
    "        # don't do any rounding of integers, leave them alone\n",
    "        return str(f)\n",
    "    if f >= 1000:\n",
    "        # numbers > 1000 just round to the nearest integer\n",
    "        s = f'{f:.0f}'\n",
    "    else:\n",
    "        # otherwise show 4 significant figures, regardless of decimal spot\n",
    "        s = f'{f:.4g}'\n",
    "    # replace leading 0's with blanks for easier reading\n",
    "    # example:  -0.32 to -.32\n",
    "    s = s.replace('-0.', '-.')\n",
    "    if s.startswith('0.'):\n",
    "        s = s[1:]\n",
    "    # Add the trailing 0's to always show 4 digits\n",
    "    # example: .32 to .3200\n",
    "    if s[0] == '.' and len(s) < 5:\n",
    "        s += '0' * (5 - len(s))\n",
    "    return s\n",
    "\n",
    "\n",
    "def general_formatter(value):\n",
    "    if isinstance(value, list) or isinstance(value, np.ndarray):\n",
    "        # apply formatting to each element\n",
    "        return '[{}]'.format(','.join([float_formatter(item) for item in value]))\n",
    "    if isinstance(value, dict):\n",
    "        return '[{}]'.format(','.join(list(value.items())))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--trainpath', '-trp',\n",
    "                        required=True,\n",
    "                        type=str, )\n",
    "    parser.add_argument('--testpath', '-ttp',\n",
    "                        required=True,\n",
    "                        type=str, )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_filepath = os.path.normpath(args.trainpath)\n",
    "    assert os.path.isfile(train_filepath), f'Could not find train file. Got {train_filepath}'\n",
    "\n",
    "    test_filepath = os.path.normpath(args.testpath)\n",
    "    assert os.path.isfile(test_filepath), f'Could not find test file. Got {test_filepath}'\n",
    "\n",
    "    data_dir = os.path.dirname(test_filepath)\n",
    "\n",
    "    logging.info(f\"\"\"Running baseline with following configuration: \n",
    "                 Train: {train_filepath} \n",
    "                 Test: {test_filepath}\"\"\")\n",
    "    run_sbert_lr_baseline(data_dir=data_dir,\n",
    "                          test_filepath=test_filepath,\n",
    "                          train_filepath=train_filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "checkthat-subjectivity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
