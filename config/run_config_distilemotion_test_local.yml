data:
  dir: /Users/maximilianheil/OMSCS/07_CLEF/Task1/checkthat-2025-subject/data
  train_en: raw_data/english/train_en.tsv
  train_en_embedding: embeddings/english/train_en_embed
  fine_tuned_model_path: fine_tuned_models/english
  val_en: raw_data/english/dev_en.tsv
  test_en: raw_data/english/test_en_unlabeled.tsv

train:
  train_test_split: 0.8
  batch_size: 100 # used for 02-nli
  dataloader_num_workers: 0 # Number of subprocesses to use for data loading
  seed: 42
  epochs: 10
  learning_rate: 0.00002
  eps: 0.00000001
  early_stopping_patience: 2
  rounding_metrics: 4
  warmup_steps: 0
  step_per: "epoch" # ("epoch", "batch", "venktesh" -> no steps)
  loss: "cross_entropy" # ("cross_entropy", "focal")
  focal_loss_gamma: 2.0 # (only used if loss is focal)

encoder_model: 
  name: "j-hartmann/emotion-english-distilroberta-base" # roberta-large #sentence-transformers/all-MiniLM-L12-v2 #"modernbert-large"
  TOKENIZERS_PARALLELISM: false # Suppress tokenizing in parallel to avoid issues when forking processes for the dataloader
  add_special_tokens: true
  padding: "max_length"
  truncation: true
  return_attention_mask: true
  return_tensors: "pt"
  pad_to_max_length: true
  r2l: false # Tokenization (True, False)
  max_length: 128
  batch_size: 20
  hidden_dim: 768
  freeze_encoder: null # whole, "first_5_layers"
  dropout_ratio: 0.1
  mlp_dim: 256
  lora_rank: null  # (recommended: 4 - 8)
  lora_alpha: null # (recommended: 2x lora_rank)

dim_red_model:
  name: "umap"
  n_neighbors: 15
  n_components: 2
  metric: euclidean
  random_state: 42

visualization:
  fig_dir: "/storage/coda1/p-dsgt_clef2025/0/shared/checkthat-2025-subjectiv-data/fig"
  data_type: "png"
  figure_size: [6, 6]